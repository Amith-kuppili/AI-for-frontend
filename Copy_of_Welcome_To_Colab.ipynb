{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amith-kuppili/AI-for-frontend/blob/main/Copy_of_Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0dl7wyHMvLZ",
        "outputId": "b1ffc2bb-1c43-4386-f548-17488bdf5e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\n",
            "\u001b[1m3423204/3423204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Text_file\n"
          ]
        }
      ],
      "source": [
        "from typing import Text\n",
        "import tensorflow as tf\n",
        "\n",
        "Text_file=tf.keras.utils.get_file(\n",
        "    fname='fra-eng.zip',\n",
        "    origin='http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip',\n",
        "    extract=True\n",
        ")\n",
        "print('Text_file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOtCFh0KQ8ZA",
        "outputId": "15a50f6c-ebce-4a4f-a2f1-9afddf5951ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.keras/datasets/fra-eng_extracted/fra.txt\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "extracted_dir = pathlib.Path(Text_file).parent\n",
        "Text_file = extracted_dir / 'fra-eng_extracted' / 'fra.txt'\n",
        "print(Text_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zyjhMc-SRV9k"
      },
      "outputs": [],
      "source": [
        "with open(Text_file) as fp:\n",
        "    test_pair=[line for line in fp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c3a6243",
        "outputId": "99f2b575-eae6-4f0a-923a-badf95a65fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fra-eng.zip', 'fra-eng_extracted']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir('/root/.keras/datasets/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0e15be0",
        "outputId": "830d7075-b115-4620-a6ed-b7aea4b8d989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['_about.txt', 'fra.txt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir('/root/.keras/datasets/fra-eng_extracted/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_GBi_5RTJX7",
        "outputId": "fa915656-6ae8-4c7f-b5d7-f9a355d19c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We need to do this right.\tOn doit faire ça bien.\n",
            "\n",
            "Give me an example.\tFournissez-moi un exemple.\n",
            "\n",
            "I used to love that.\tJ'adorais ça.\n",
            "\n",
            "Despite everything, Tom started to relax.\tMalgré tout, Tom commença à se détendre.\n",
            "\n",
            "I read a book while eating.\tJ'ai lu un livre en mangeant.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "for _ in range(5):\n",
        "    print(random.choice(test_pair))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3FEUFP1uT_fH"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def normalize(text):\n",
        "  # Split the line into English and French parts first\n",
        "  eng, fre = text.strip().lower().split('\\t')\n",
        "\n",
        "  # Normalize and clean the English part\n",
        "  eng = unicodedata.normalize('NFKC', eng)\n",
        "  eng = re.sub(r'([,.!?])', r' \\1', eng)\n",
        "  eng = re.sub(r'[^a-zA-Z,.!?]+', ' ', eng)\n",
        "  eng = re.sub(r'\\s+', ' ', eng).strip()\n",
        "\n",
        "  # Normalize and clean the French part\n",
        "  fre = unicodedata.normalize('NFKC', fre)\n",
        "  fre = re.sub(r'([,.!?])', r' \\1', fre)\n",
        "  fre = re.sub(r'[^a-zA-Z,.!?]+', ' ', fre)\n",
        "  fre = re.sub(r'\\s+', ' ', fre).strip()\n",
        "\n",
        "  # Add start and end tokens to the French part\n",
        "  fre = '[start] ' + fre + ' [end]'\n",
        "\n",
        "  return eng, fre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6ugm19VzT_cQ"
      },
      "outputs": [],
      "source": [
        "with open(Text_file) as fp:\n",
        "    test_pair=[normalize(line) for line in fp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw2W4cCoY0V9",
        "outputId": "15b3d5be-a056-4980-aa03-6be55c501339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('i seldom eat dairy products .', '[start] je mange rarement de produits laitiers . [end]')\n",
            "('don t mind her .', '[start] ne te soucie pas d elle . [end]')\n",
            "('i cannot look at this photo without being reminded of my school days .', '[start] je ne peux pas regarder cette photo sans que cela me rappelle l poque o j allais l cole . [end]')\n",
            "('i really want to see tom today .', '[start] je veux vraiment voir tom aujourd hui . [end]')\n",
            "('i would like a cup of coffee .', '[start] j appr cierais une tasse de caf . [end]')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "  print(random.choice(test_pair))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGlyjzglZQgO",
        "outputId": "0b0f7e5a-1c9b-45b7-b5a7-e2139d7ce788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique English tokens: 13749\n",
            "Number of unique French tokens: 19770\n",
            "Maximum English sentence length: 52\n",
            "Maximum French sentence length: 68\n"
          ]
        }
      ],
      "source": [
        "eng_tokens_set, fre_tokens_set = set(), set()\n",
        "eng_maxlen, fre_maxlen = 0, 0\n",
        "\n",
        "for eng, fre in test_pair:\n",
        "  eng_tokens = eng.split()\n",
        "  fre_tokens = fre.split()\n",
        "\n",
        "  eng_tokens_set.update(eng_tokens)\n",
        "  fre_tokens_set.update(fre_tokens)\n",
        "\n",
        "  eng_maxlen = max(eng_maxlen, len(eng_tokens))\n",
        "  fre_maxlen = max(fre_maxlen, len(fre_tokens))\n",
        "\n",
        "print(f\"Number of unique English tokens: {len(eng_tokens_set)}\")\n",
        "print(f\"Number of unique French tokens: {len(fre_tokens_set)}\")\n",
        "print(f\"Maximum English sentence length: {eng_maxlen}\")\n",
        "print(f\"Maximum French sentence length: {fre_maxlen}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_1bEylQOciWu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('text_pairs.pickle','wb') as fp:\n",
        "  pickle.dump(test_pair,fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aUgUROXckeY8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import pickle\n",
        "\n",
        "# Load preprocessed text pairs\n",
        "with open(\"text_pairs.pickle\", 'rb') as fp:\n",
        "    test_pair = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PYX7__wlknVF"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data\n",
        "import random\n",
        "random.shuffle(test_pair)\n",
        "\n",
        "# Split into train and test sets\n",
        "n_val = int(0.15 * len(test_pair))\n",
        "n_train = len(test_pair) - 2 * n_val\n",
        "train_pair = test_pair[:n_train]\n",
        "val_pair = test_pair[n_train: n_train + n_val]\n",
        "test_pair = test_pair[n_train + n_val:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary sizes and sequence length\n",
        "vocab_en = 10000\n",
        "vocab_fr = 20000\n",
        "seq_length = 25\n",
        "\n",
        "# Initialize TextVectorization layers\n",
        "eng_vect = TextVectorization(\n",
        "    max_tokens=vocab_en,\n",
        "    standardize=None,\n",
        "    split='whitespace',\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length\n",
        ")\n",
        "\n",
        "fre_vect = TextVectorization(\n",
        "    max_tokens=vocab_fr,\n",
        "    standardize=None,\n",
        "    split='whitespace',\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length + 1  # +1 for start token\n",
        ")\n",
        "\n",
        "# Adapt TextVectorization layers to training data\n",
        "train_eng = [pair[0] for pair in train_pair]\n",
        "train_fre = [pair[1] for pair in train_pair]\n",
        "\n",
        "eng_vect.adapt(train_eng)\n",
        "fre_vect.adapt(train_fre)\n",
        "\n",
        "# Serialize the vectorization layers and training/test data\n",
        "with open('vectorize.pickle', 'wb') as fp:\n",
        "    data = {'train': train_pair,\n",
        "            'test': test_pair,\n",
        "            'eng_vect': eng_vect.get_config(),\n",
        "            'fre_vect': fre_vect.get_config(),\n",
        "            'eng_weights': eng_vect.get_weights(),\n",
        "            'fre_weights': fre_vect.get_weights()\n",
        "            }\n",
        "    pickle.dump(data, fp)\n",
        "\n",
        "# Load serialized data\n",
        "with open(\"vectorize.pickle\", 'rb') as fp:\n",
        "    data = pickle.load(fp)\n",
        "\n",
        "# Retrieve train and test pairs\n",
        "train_pair = data['train']\n",
        "test_pair = data['test']\n",
        "\n",
        "# Reconstruct TextVectorization layers\n",
        "eng_vect = TextVectorization.from_config(data['eng_vect'])\n",
        "eng_vect.set_weights(data['eng_weights'])\n",
        "fre_vect = TextVectorization.from_config(data['fre_vect'])\n",
        "fre_vect.set_weights(data['fre_weights'])\n",
        "\n",
        "# Define function to format dataset\n",
        "def format_dataset(eng, fre):\n",
        "    eng = eng_vect(eng)\n",
        "    fre = fre_vect(fre)\n",
        "    source = {'encode_inp': eng,\n",
        "              'decode_inp': fre[:, :-1]\n",
        "              }\n",
        "    target = fre[:, 1:]\n",
        "    return (source, target)\n",
        "\n",
        "# Define function to create dataset\n",
        "def make_dataset(pairs, batchsize=64):\n",
        "    eng_text, fre_text = zip(*pairs)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_text), list(fre_text)))\n",
        "    return dataset.shuffle(2048).batch(batchsize).map(format_dataset).prefetch(16)#.cache()\n",
        "\n",
        "# Create TensorFlow datasets for training and testing\n",
        "train_ds = make_dataset(train_pair)\n",
        "test_ds = make_dataset(test_pair)"
      ],
      "metadata": {
        "id": "39T9XuakYZ7Q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Embeddings**"
      ],
      "metadata": {
        "id": "DpAYDmo1bchB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to generate positional encoding matrix\n",
        "def pos_enc_matrix(L, d, n=10000):\n",
        "    assert d % 2 == 0\n",
        "    d2 = d // 2\n",
        "\n",
        "    P = np.zeros((L, d))\n",
        "    k = np.arange(L).reshape(-1, 1)\n",
        "    i = np.arange(d2).reshape(1, -1)\n",
        "\n",
        "    denom = np.power(n, -i / d2)\n",
        "    args = k * denom\n",
        "\n",
        "    P[:, ::2] = np.sin(args)\n",
        "    P[:, 1::2] = np.cos(args)\n",
        "    return P\n",
        "\n",
        "# Custom Keras layer for positional embedding\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, seq_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
        "        matrix = pos_enc_matrix(seq_length, embed_dim)\n",
        "\n",
        "        self.positional_embedding = tf.constant(matrix, dtype='float32')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        return embedded_tokens + self.positional_embedding\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'seq_length': self.seq_length,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim\n",
        "        })\n",
        "\n",
        "# Usage and Validation\n",
        "vocab_en = 10000\n",
        "seq_length = 25\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    embed_en = PositionalEmbedding(seq_length, vocab_en, embed_dim=512)\n",
        "    en_emb = embed_en(inputs['encode_inp'])\n",
        "    print(en_emb._keras_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "rjb9Y9pZbn-h",
        "outputId": "7273099c-aecf-4899-ccdb-00c550fa55ff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to MapDataset:3 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::MemoryCacheImpl::Prefetch::Map: Table not initialized.\n\t [[{{node text_vectorization_1_2/None_Lookup/LookupTableFindV2}}]] [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2143251905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0membed_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0men_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encode_inp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to MapDataset:3 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::MemoryCacheImpl::Prefetch::Map: Table not initialized.\n\t [[{{node text_vectorization_1_2/None_Lookup/LookupTableFindV2}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}